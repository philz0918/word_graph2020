{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_book_20_cen = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(200) :\n",
    "    f  = open(\"20/20_\"+str(idx)+\".txt\", \"r\", encoding ='utf-8')\n",
    "    content = f.read()\n",
    "    list_book_20_cen.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_book_20_cen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_book_19_cen= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for idx in range(200) :\n",
    "    f = open(\"19/19_\"+str(idx)+ \".txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "    content = f.read()\n",
    "    list_book_19_cen.append(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_book_19_cen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenize() :\n",
    "    \n",
    "    def __init__(self, book_list) :\n",
    "        self.book_list = book_list\n",
    "        \n",
    "    def token_process(self) :\n",
    "        # tokenize list\n",
    "        t_list =[]\n",
    "        \n",
    "        for contents in self.book_list :\n",
    "            \n",
    "            #lower all character (regulization)\n",
    "            contents = str(contents).lower()\n",
    "           \n",
    "            t_list.append(contents)\n",
    "            \n",
    "        #tokenize with removing punctuation\n",
    "        #remove stop word\n",
    "        #lemmatization\n",
    "         \n",
    "        regexp_tokenize = RegexpTokenizer(r'\\w+')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token_sentence =[]\n",
    "        for words in t_list :\n",
    "            \n",
    "            word_tokens = regexp_tokenize.tokenize(words)\n",
    "            filter_sentence =[ fw  for fw in word_tokens if not fw in stop_words ]\n",
    "            filter_sentence =[ lemmatizer.lemmatize(lw) for lw in filter_sentence ]\n",
    "            token_sentence.append(filter_sentence)\n",
    "         \n",
    "        #add postag\n",
    "        #token_sentence = nltk.pos_tag(token_sentence)\n",
    "        \n",
    "        return token_sentence\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Counting():\n",
    "    \n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "        self.n = len(word_list)\n",
    "    \n",
    "    def word_cnt (self) :\n",
    "        \n",
    "        cnt = {}\n",
    "        for idx in range(self.n) :\n",
    "            for idx2 in range(len(self.word_list[idx])):\n",
    "                word = self.word_list[idx][idx2]\n",
    "                if word not in cnt.keys() :\n",
    "                    cnt[word] = 1 \n",
    "                \n",
    "                else :\n",
    "                    cnt[word] += 1 \n",
    "        \n",
    "        return cnt \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token = Tokenize(list_book_20_cen)\n",
    "\n",
    "list_of_20_cen = token.token_process()\n",
    "\n",
    "\n",
    "#print(list_of_20_cen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = Counting(list_of_20_cen)\n",
    "\n",
    "w_list_20 = count.word_cnt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "order_dict_20 = sorted(w_list_20.items(),key=lambda x: x[1] ,reverse = True)\n",
    "#print(order_dict_20)\n",
    "\n",
    "\n",
    "over_100_20_cen = [order_dict_20[i] for i in range(len(order_dict_20)) if order_dict_20[i][1] >100]\n",
    "#print( over_100_20_cen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df =pd.DataFrame(over_100_20_cen, columns=['Word','Frequency'])\n",
    "df.to_csv('20_cen.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token = Tokenize(list_book_19_cen)\n",
    "\n",
    "list_of_19_cen = token.token_process()\n",
    "\n",
    "#print(list_of_19_cen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = Counting(list_of_19_cen)\n",
    "w_list_19 = count.word_cnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_dict_19 = sorted(w_list_19.items(), key= lambda x : x[1], reverse = True)\n",
    "\n",
    "over_100_19_cen = [order_dict_19[i] for i in range(len(order_dict_19)) if order_dict_19[i][1] >100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(over_100_19_cen, columns=['Word', 'Frequency'])\n",
    "df2.to_csv('19_cen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Frequency histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df2.plot(kind='bar', x='Word', y='Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
